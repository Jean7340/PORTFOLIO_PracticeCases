{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming for Analytics -- Homework Assignment 1\n",
    "## \"Belgium ATMs\"\n",
    "\n",
    "### SOLUTION KEY - ELABORATE VERSION\n",
    "\n",
    "#### Name: Yufeng Huang\n",
    "#### Email: yufeng.huang@simon.rochester.edu\n",
    "#### Date: 7/30/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. General description and preparation steps\n",
    "\n",
    "This is the first homework assignment for GBA464 Programming for Analytics. Here, we will read and analyze a dataset on the number of ATMs in each market and some withdrawal measures. We have not learned to work with Pandas yet. So we we will work with NumPy and a dataset in the form of a list, where each element in the list is a column name:column value pair. See Exercise 1 in the NumPy lecture notes for a similar data structure. \n",
    "\n",
    "To complete the assignment, you should:\n",
    "1. Make sure your Pandas and NumPy are installed correctly. We have installed NumPy in the first week. In the same manner, install Pandas using `pip install pandas` (or `conda install pandas`) in the Terminal/Anaconda Prompt.\n",
    "2. Follow the notebook and complete each question. Do not alter my code in Section 0 (\"General description and preparation steps\"). \n",
    "3. When your code is finished, clear all outputs and select \"run all\" to do a full run. Then check and confirm there is no error, and all outputs are as intended. \n",
    "4. Submit the finished notebook on Blackboard. Do this before the submission deadline. \n",
    "\n",
    "Grading is based on completion. So make a genuine programming attempt in completing each question. Do not \"hard code\" your answer. For example, the number of rows in the data is 659, and the number of nonmissing rows is 310. Do **not** code `nrows = 659` and `nrows_nonmissing = 310` and use them as answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data\n",
    "\n",
    "We will first read data. The data comes from a csv file in a url. We will use Pandas to read data. But we will not use Pandas after the conversion. So do not modify the first few lines of code, where I read the data, convert it into a dictionary, and delete the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>numATMs</th>\n",
       "      <th>ATMwithdr</th>\n",
       "      <th>withdrvalue</th>\n",
       "      <th>unemprate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25542593</td>\n",
       "      <td>79.13402557</td>\n",
       "      <td>0.072868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7006</td>\n",
       "      <td>2</td>\n",
       "      <td>1.837865114</td>\n",
       "      <td>102.6663437</td>\n",
       "      <td>0.022695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4234</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6229</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.024402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10303</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606253982</td>\n",
       "      <td>98.93833923</td>\n",
       "      <td>0.028438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.021766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.021766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2033</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.021766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>15521</td>\n",
       "      <td>2</td>\n",
       "      <td>0.698489904</td>\n",
       "      <td>110.1268387</td>\n",
       "      <td>0.023195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>5941</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.023195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     population  numATMs    ATMwithdr  withdrvalue  unemprate\n",
       "0          3722        1   0.25542593  79.13402557   0.072868\n",
       "1          7006        2  1.837865114  102.6663437   0.022695\n",
       "2          4234        0      missing      missing   0.027397\n",
       "3          6229        0      missing      missing   0.024402\n",
       "4         10303        1  0.606253982  98.93833923   0.028438\n",
       "..          ...      ...          ...          ...        ...\n",
       "654         601        0      missing      missing   0.021766\n",
       "655        1028        0      missing      missing   0.021766\n",
       "656        2033        0      missing      missing   0.021766\n",
       "657       15521        2  0.698489904  110.1268387   0.023195\n",
       "658        5941        0      missing      missing   0.023195\n",
       "\n",
       "[659 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read df from Google Drive\n",
    "df = pd.read_csv('https://drive.google.com/uc?id=1ouWC0rdnuKevZGiyT6w2YJgT25Wb2_oj&export=download')\n",
    "\n",
    "# Note: if cannot access this link or it is too slow, download file \"belgium_atm.csv\" from Blackboard and replace the link with the file's location\n",
    "\n",
    "# print df for view\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the Pandas object `df` into a dictionary, which has the format column_name:column_value. Specifically, each column we made it a NumPy `ndarray`. We call this dictionary `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into a dictionary, treating each element is a NumPy Array\n",
    "data = {}\n",
    "for col in df.columns:\n",
    "    data[col] = np.array(df[col].tolist())\n",
    "\n",
    "# we can also just run below, but we'll need to separately convert the columns into NumPy Arrays\n",
    "# data = df.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we confirm `data` is what we wanted:\n",
    "- It is a dictionary. \n",
    "- It has columns \"population,\" \"numATMs,\" \"ATMwithdr,\" \"withdrvalue,\" \"unemprate.\"\n",
    "- Each column is a NumPy `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['population', 'numATMs', 'ATMwithdr', 'withdrvalue', 'unemprate'])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# we can show the structure of data\n",
    "print(type(data))       # it's a dict.\n",
    "print(data.keys())      # it has columns population, numATMs, ATMwithdr, withdrvalue, unemprate, and numbranches.\n",
    "print(type(data[\"population\"]))     # columns are ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, our data reading job is done, so we delete df and never touch it\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the data's structure\n",
    "We first try to gain understandings of the dataset. Use what we have learned (basic Python functions, NumPy, and flow control if necessary), find out about the following:\n",
    "- [1a] How many rows does the dataset have? Keep in mind the number of rows is the length of each column. We often call each row in a dataset \"an observation.\"\n",
    "- [1b] How many columns are numeric -- meaning that their values are either integer or float? How many columns are strings? Which columns are strings? How many columns are booleans? Keep in mind that with NumPy each `ndarray` has a fixed type, and that is captured in the attribute `.dtype`.\n",
    "- [1c] How many rows contain a string \"missing\" in any column? If we think \"missing\" represents missing values, how many rows have any missing values?\n",
    "\n",
    "Keep in mind we will not use Pandas functions to answer these and the subsequent questions. Also, although we are working with dictionaries, we will still call each element a \"column.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows is 659\n"
     ]
    }
   ],
   "source": [
    "# Code for 1a\n",
    "nrows = len(data[\"population\"])\n",
    "print(f\"the number of rows is {nrows}\")\n",
    "\n",
    "# just say len(data[\"population\"]) or something similar is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "There are several alternatives to get shapes, size, and dimensions. E.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1a alternatives\n",
    "(nrows,) = data[\"population\"].shape\n",
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a alternatives (if we were to use df)\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns = 5, number of boolean columns = 0, number of string columns = 2, and the string columns are ['ATMwithdr', 'withdrvalue']\n"
     ]
    }
   ],
   "source": [
    "# Code for 1b\n",
    "ncol = 0\n",
    "ncol_num = 0\n",
    "ncol_bool = 0\n",
    "ncol_str = 0\n",
    "set_string_col = []\n",
    "for col in data.keys():\n",
    "    ncol += 1   # always have one more string row\n",
    "    if data[col].dtype in [\"float\", \"int\"]:\n",
    "        ncol_num += 1\n",
    "    elif data[col].dtype == \"bool\":\n",
    "        ncol_bool += 1\n",
    "    else:\n",
    "        ncol_str += 1\n",
    "        set_string_col.append(col)\n",
    "\n",
    "print(f\"number of columns = {ncol}, number of boolean columns = {ncol_bool}, number of string columns = {ncol_str}, and the string columns are {set_string_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "This was a really tricky question. The intent was simple: check types. At the time of making the question, I didn't realize at the time that `.dtype == \"str\"` doesn't work as intended. Of course I realized this before I release the assignment, but I chose to keep the question because we can always find \"work arounds.\" See above, where I avoided checking strings. \n",
    "\n",
    "Now, the real question from some of you is \"why couldn't we say 'np.array.dtype == str'?\" \n",
    "\n",
    "A quick-ish answer is that strings have many specific types, and these are related to their encoding. If we see dtype is \"U<11\" that means we have Unicode strings up to 11 characters. But there are other string encodings, such as \"ASCII.\"\n",
    "\n",
    "One way to tackle this head-on is to use `np.issubdtype()` to get True's for something that is a subtype of the broader family of NumPy strings, `np.str_`. We can also change the \"int\" and \"bool\" to be NumPy versions of these types.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns = 5, number of boolean columns = 0, number of string columns = 2, and the string columns are ['ATMwithdr', 'withdrvalue']\n"
     ]
    }
   ],
   "source": [
    "ncol = 0\n",
    "ncol_num = 0\n",
    "ncol_bool = 0\n",
    "ncol_str = 0\n",
    "set_string_col = []\n",
    "for col in data.keys():\n",
    "    ncol += 1   # always have one more string row\n",
    "    if data[col].dtype in {np.int_, np.float_}:\n",
    "        ncol_num += 1\n",
    "    elif data[col].dtype == np.bool_:\n",
    "        ncol_bool += 1\n",
    "    elif np.issubdtype(data[col].dtype, np.str_):\n",
    "        ncol_str += 1\n",
    "        set_string_col.append(col)\n",
    "\n",
    "print(f\"number of columns = {ncol}, number of boolean columns = {ncol_bool}, number of string columns = {ncol_str}, and the string columns are {set_string_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows with a 'missing' is 349, which is also the number of rows with missing values\n"
     ]
    }
   ],
   "source": [
    "# Code for 1c\n",
    "is_missing_row = (data['ATMwithdr'] == \"missing\") | (data['withdrvalue'] == \"missing\")  # we need is_missing_row later\n",
    "missing_rows = is_missing_row.sum()     # True's are 1's, so they sum up to the number of missing rows\n",
    "print(f\"the number of rows with a 'missing' is {missing_rows}, which is also the number of rows with missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "It is interesting to see that people have very different responses to this question and also question 2.\n",
    "\n",
    "One might be tempted to loop across rows. Afterall, wouldn't the way to count \"missing\" be just to add one to a counter every time we see \"missing\"? \n",
    "\n",
    "But (remember the drunk bar guy) with np.arrays, we can avoid loops with universal functions (vectorized functions). In this case, \"==\" is vectorized, so we can just feed the entire column to the operation at once. \n",
    "\n",
    "In general, because we have pd.Series or np.ndarrays to represent columns (a column is a pd.Series or an np.array or elements), we do not have to loop over rows most of the time. The exception is we do not have a vectorized function, and we cannot write one ourselves. \n",
    "\n",
    "Writing loops is not only time consuming but can also lead to slower code. To see this point, let's use `%timeit` and bechmark the two versions. We will return to this point around the end of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 Âµs Â± 276 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# benchmark, sum way\n",
    "def vectorized_approach():\n",
    "    is_missing_row = (data['ATMwithdr'] == \"missing\") | (data['withdrvalue'] == \"missing\")  # we need is_missing_row later\n",
    "    missing_rows = is_missing_row.sum()\n",
    "\n",
    "%timeit vectorized_approach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552 Âµs Â± 17.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# benchmark, for loop\n",
    "def loop_approach():\n",
    "    missing_rows = 0\n",
    "    for r in range(nrows):\n",
    "        if (data['ATMwithdr'][r] == \"missing\") | (data['withdrvalue'][r] == \"missing\"): \n",
    "            missing_rows += 1\n",
    "\n",
    "%timeit loop_approach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What explains the rows with \"missing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that the string \"missing\" was an intention to represent missing values. Let's get a smaller dataset that focuses on rows without any missings. \n",
    "- [2a] First, create a **copy** of the original dictionary `data`, call it `data_sub`. \n",
    "- [2b] Then, for each column in `data_sub`, remove rows where **any column** contains \"missing.\" For example: \n",
    "    - for row 0, we have population = 3722, numATMs = 1, ATMwithdr = 0.255, withdrvalue = 79.134, unemprate = 0.073. None of them is a zero, so this row stays in the new dictionary `data_sub`. \n",
    "    - for row 2, population = 4234, numATMs = 0, ATMwithdr = \"missing\", withdrvalue = \"missing\", and unemprate = 0.027. Some of the columns in this row have the value \"missing,\" so we'll drop the entire row. \n",
    "    - the new dictionary should still have five elements, each being a NumPy `ndarray` with equal length. Print that length. \n",
    "- [2c] For this smaller data, compute the average of population, numATMs, and unemprate. Compare these averages against the full sample's average. Organize your results in a convenient format (a format that you like). What variable explains the missing rows? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for 2a\n",
    "data_sub = data.copy()      # we don't actually need this step. We can just create an empty dictionary and add the shrunk columns in. But the instruction says copy, which is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_sub now has 310 rows\n"
     ]
    }
   ],
   "source": [
    "# Code for 2b\n",
    "#   now we use is_missing_row to do indexing, or masking. You can convert it to indices using np.where and use the numeric indices\n",
    "for col in data.keys():\n",
    "    data_sub[col] = data_sub[col][~is_missing_row]\n",
    "    if col in set_string_col:                               # use the set of string columns, derived before\n",
    "        data_sub[col] = data_sub[col].astype(\"float\")       # if we're getting the wrong rows, i.e. if the rows still contain missing, there will be an error here\n",
    "\n",
    "nrows_sub = len(data_sub[\"population\"])\n",
    "print(f\"data_sub now has {nrows_sub} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "Similar comment as before: we often loop over columns, but we do not often loop over rows. Here, over rows we have access to boolean masking, and we can directly use `[~is_missing_row]` (find non-missing rows) to select column.\n",
    "\n",
    "Remember to select all columns for the same set of rows!\n",
    "\n",
    "Same comment for the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the full sample averages are {'population': 8738.464, 'numATMs': 0.737, 'unemprate': 0.031}\n",
      "the sub sample averages are {'population': 13445.374, 'numATMs': 1.568, 'unemprate': 0.032}\n",
      "those with non-missing observations are larger markets (higher population) and more ATMs\n"
     ]
    }
   ],
   "source": [
    "# Code for 2c (Just compare the averages. The last question is for thinking.)\n",
    "avg_full = {}       # use dictionary because they have keys\n",
    "avg_sub = {}\n",
    "\n",
    "for col in [\"population\", \"numATMs\", \"unemprate\"]:\n",
    "    avg_full[col] = data[col].mean().round(3)   # round to make the display nicer. Note the method chaining.\n",
    "    avg_sub[col] = data_sub[col].mean().round(3)\n",
    "\n",
    "print(f\"the full sample averages are {avg_full}\")\n",
    "print(f\"the sub sample averages are {avg_sub}\")\n",
    "print(\"those with non-missing observations are larger markets (higher population) and more ATMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fill in missing with zeros\n",
    "\n",
    "We now see that the \"missing\"'s come from the fact that there is no ATMs in certain markets. That is, when ATMwithdr and withdrvalue is \"missing,\" numATMs is 0. So one way to keep all rows is to replace the value \"missing\" with some other values. \n",
    "\n",
    "- [3a] Convert the observations in ATMwithdr and withdrvalue, whose current value is \"missing,\" to \"0.0\" (string 0.0). Then, convert the entire columns to float. Do so directly on the dictionary `data`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to confirm, the two columns have types [dtype('float64'), dtype('float64')]\n"
     ]
    }
   ],
   "source": [
    "# Code for 3a\n",
    "types = []\n",
    "for col in [\"ATMwithdr\", \"withdrvalue\"]:\n",
    "    data[col][is_missing_row] = \"0.0\"\n",
    "    data[col] = data[col].astype(\"float\")\n",
    "    types.append(data[col].dtype)\n",
    "\n",
    "print(f\"to confirm, the two columns have types {types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final comment\n",
    "\n",
    "I talked to many people and saw many of you working very hard. Not everyone does this in the most ideal way on the first try. That's okay! We learn by accumulating experiences. Good job! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gba464",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
